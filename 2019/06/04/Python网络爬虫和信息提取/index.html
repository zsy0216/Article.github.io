<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="python,网络爬虫,">










<meta name="description" content="1.Requests库入门Requests安装用管理员身份打开命令提示符： 1pip install requests  测试：打开IDLE： 123456&amp;gt;&amp;gt;&amp;gt; import requests&amp;gt;&amp;gt;&amp;gt; r = requests.get(&quot;http://www.baidu.com&quot;)&amp;gt;&amp;gt;&amp;gt; r.status_code200&amp;gt;&amp;gt;&amp;gt;">
<meta name="keywords" content="python,网络爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="Python网络爬虫和信息提取">
<meta property="og:url" content="http://yoursite.com/2019/06/04/Python网络爬虫和信息提取/index.html">
<meta property="og:site_name" content="Tassel">
<meta property="og:description" content="1.Requests库入门Requests安装用管理员身份打开命令提示符： 1pip install requests  测试：打开IDLE： 123456&amp;gt;&amp;gt;&amp;gt; import requests&amp;gt;&amp;gt;&amp;gt; r = requests.get(&quot;http://www.baidu.com&quot;)&amp;gt;&amp;gt;&amp;gt; r.status_code200&amp;gt;&amp;gt;&amp;gt;">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://zsy.xyz/images/20190604/1558688318694.png">
<meta property="og:updated_time" content="2019-06-04T10:14:40.062Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python网络爬虫和信息提取">
<meta name="twitter:description" content="1.Requests库入门Requests安装用管理员身份打开命令提示符： 1pip install requests  测试：打开IDLE： 123456&amp;gt;&amp;gt;&amp;gt; import requests&amp;gt;&amp;gt;&amp;gt; r = requests.get(&quot;http://www.baidu.com&quot;)&amp;gt;&amp;gt;&amp;gt; r.status_code200&amp;gt;&amp;gt;&amp;gt;">
<meta name="twitter:image" content="https://zsy.xyz/images/20190604/1558688318694.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/06/04/Python网络爬虫和信息提取/">





  <title>Python网络爬虫和信息提取 | Tassel</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Tassel</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/04/Python网络爬虫和信息提取/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="tassel">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tassel">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Python网络爬虫和信息提取</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-04T17:55:23+08:00">
                2019-06-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/06/04/Python网络爬虫和信息提取/" class="leancloud_visitors" data-flag-title="Python网络爬虫和信息提取">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-Requests库入门"><a href="#1-Requests库入门" class="headerlink" title="1.Requests库入门"></a>1.Requests库入门</h1><h2 id="Requests安装"><a href="#Requests安装" class="headerlink" title="Requests安装"></a>Requests安装</h2><p>用管理员身份打开命令提示符：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>

<p>测试：打开IDLE：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.encoding = <span class="string">'utf-8'</span> <span class="comment">#修改默认编码</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.text		<span class="comment">#打印网页内容</span></span><br></pre></td></tr></table></figure>

<h2 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h2><p>超文本传输协议,Hypertext Transfer Protocol.</p>
<p>HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。</p>
<p>HTTP协议采用URL作为定位网络资源的标识。</p>
<h3 id="URL格式"><a href="#URL格式" class="headerlink" title="URL格式"></a>URL格式</h3><p><code>http://host[:port][path]</code></p>
<p>host:合法的Internet主机域名或IP地址</p>
<p>port：端口号，缺省端口为80</p>
<p>path：请求资源的路径</p>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>GET</td>
<td>请求获取URL位置的资源</td>
</tr>
<tr>
<td>HEAD</td>
<td>请求获取URl位置资源的响应消息报告，即获得该资源的头部信息</td>
</tr>
<tr>
<td>POST</td>
<td>请求向URL位置的资源后附加新的数据</td>
</tr>
<tr>
<td>PUT</td>
<td>请求向URL位置存储一个资源，覆盖原URL位置的资源</td>
</tr>
<tr>
<td>PATCH</td>
<td>请求局部更新URL位置的资源，即改变该处资源的部分内容</td>
</tr>
<tr>
<td>DELETE</td>
<td>请求删除URL位置存储的资源</td>
</tr>
</tbody></table>
<h2 id="Requests主要方法"><a href="#Requests主要方法" class="headerlink" title="Requests主要方法"></a>Requests主要方法</h2><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>requests.request()</td>
<td>构造一个请求，支撑以下各方法的基础方法</td>
</tr>
<tr>
<td>requests.get()</td>
<td>获取HTML网页的主要方法，对应于HTTP的GET</td>
</tr>
<tr>
<td>requests.head()</td>
<td>获取HTML网页头信息的方法，对应于HTTP的HEAD</td>
</tr>
<tr>
<td>requests.post()</td>
<td>向HTML网页提交POST请求的方法，对应于HTTP的POST</td>
</tr>
<tr>
<td>requests.put()</td>
<td>向HTML网页提交PUT请求的方法，对应于HTTP的PUT</td>
</tr>
<tr>
<td>requests.patch()</td>
<td>向HTML网页提交局部修改请求，对应于HTTP的PATCH</td>
</tr>
<tr>
<td>requests.delete()</td>
<td>向HTML网页提交删除请求，对应于HTTP的DELETE</td>
</tr>
</tbody></table>
<p>主要方法为request方法，其他方法都是在此方法基础上封装而来以便使用。</p>
<h3 id="request-方法"><a href="#request-方法" class="headerlink" title="request()方法"></a>request()方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">requests.request(method,url,**kwargs)</span><br><span class="line"><span class="comment">#method:请求方式，对应get/put/post等7种</span></span><br><span class="line"><span class="comment">#url：拟获取页面的url链接</span></span><br><span class="line"><span class="comment">#**kwargs：控制访问的参数，共13个</span></span><br></pre></td></tr></table></figure>

<p>**kwargs：控制访问的参数，均为可选项</p>
<h3 id="get-方法"><a href="#get-方法" class="headerlink" title="get()方法"></a>get()方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">r  = requests.get(url)</span><br><span class="line">完整方法：</span><br><span class="line">requests.get(url,params=<span class="literal">None</span>,**kwargs)</span><br><span class="line">	url:拟获取页面的url链接</span><br><span class="line">	params:url中的额外参数，字典或字节流格式，可选</span><br><span class="line">	**kwargs:<span class="number">12</span>个控制访问的参数，可选</span><br></pre></td></tr></table></figure>

<p>get()方法：</p>
<p>构造一个向服务器请求资源的Request对象</p>
<p>返回一个包含服务器资源的Response对象</p>
<h4 id="Response对象"><a href="#Response对象" class="headerlink" title="Response对象"></a>Response对象</h4><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.status_code</td>
<td>HTTP请求的返回状态，200表示连接成功，404表示失败</td>
</tr>
<tr>
<td>r.text</td>
<td>HTTP响应内容的字符串形式，即：url对应的页面内容</td>
</tr>
<tr>
<td>r.encoding</td>
<td>从HTTP header中猜测的响应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的响应内容编码方式（备选编码方式）</td>
</tr>
<tr>
<td>r.content</td>
<td>HTTP响应内容的二进制形式</td>
</tr>
</tbody></table>
<h3 id="head-方法"><a href="#head-方法" class="headerlink" title="head()方法"></a>head()方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = requests.head(&apos;http://httpbin.org/get&apos;)</span><br><span class="line">r.headers</span><br></pre></td></tr></table></figure>

<p>获取网络资源的概要信息</p>
<h3 id="post-方法"><a href="#post-方法" class="headerlink" title="post()方法"></a>post()方法</h3><p>向服务器提交新增数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">payload = &#123;<span class="string">'key1'</span>:<span class="string">'value1'</span>,<span class="string">'key2'</span>:<span class="string">'value2'</span>&#125; <span class="comment">#新建一个字典</span></span><br><span class="line"><span class="comment">#向URL POST一个字典，自动编码为form（表单）</span></span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>,data = payload)</span><br><span class="line"><span class="comment">#向URL POST一个字符串，自动编码为data</span></span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>,data = <span class="string">'ABC'</span>) </span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>

<h3 id="put-方法"><a href="#put-方法" class="headerlink" title="put()方法"></a>put()方法</h3><p>同post，只不过会把原来的内容覆盖掉。</p>
<h3 id="patch-方法"><a href="#patch-方法" class="headerlink" title="patch()方法"></a>patch()方法</h3><h3 id="delete-方法"><a href="#delete-方法" class="headerlink" title="delete()方法"></a>delete()方法</h3><h2 id="Requests库的异常"><a href="#Requests库的异常" class="headerlink" title="Requests库的异常"></a>Requests库的异常</h2><table>
<thead>
<tr>
<th align="left">异常</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">requests.ConnectionError</td>
<td>网络连接错误异常，如DNS查询失败、拒绝连接等</td>
</tr>
<tr>
<td align="left">requests.HTTPError</td>
<td>HTTP错误异常</td>
</tr>
<tr>
<td align="left">requests.URLRequired</td>
<td>URL缺失异常</td>
</tr>
<tr>
<td align="left">requests.TooManyRedirects</td>
<td>超过最大 重定向次数，产生重定向异常</td>
</tr>
<tr>
<td align="left">requests.ConnectTimeout</td>
<td>连接远程服务器超时异常</td>
</tr>
<tr>
<td align="left">requests.Timeout</td>
<td>请求URL超时，产生超时异常</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>异常方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.raise_for_status</td>
<td>如果不是200产生异常requests.HTTPError</td>
</tr>
</tbody></table>
<h2 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url,timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status() <span class="comment">#如果不是200，引发HTTPError异常</span></span><br><span class="line">        r.recoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"产生异常"</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">    print(getHTMLText(url))</span><br></pre></td></tr></table></figure>

<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><h3 id="向百度提交关键词"><a href="#向百度提交关键词" class="headerlink" title="向百度提交关键词"></a>向百度提交关键词</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向搜索引擎进行关键词提交</span></span><br><span class="line">url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    kv = &#123;<span class="string">'wd'</span>:<span class="string">'python'</span>&#125;</span><br><span class="line">    r = requests.get(url,params =kv)</span><br><span class="line">    print(r.request.url)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    print(len(r.text))</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"产生异常"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="获取网络图片及存储"><a href="#获取网络图片及存储" class="headerlink" title="获取网络图片及存储"></a>获取网络图片及存储</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">url = <span class="string">"http://image.ngchina.com.cn/2019/0423/20190423024928618.jpg"</span></span><br><span class="line">root = <span class="string">"D://2345//Temp//"</span></span><br><span class="line">path = root + url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(root):</span><br><span class="line">        os.mkdir(root)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(r.content)  <span class="comment">#r.content返回二进制内容</span></span><br><span class="line">            f.close()</span><br><span class="line">            print(<span class="string">"文件保存成功"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"文件已存在"</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"爬取失败"</span>)</span><br></pre></td></tr></table></figure>

<h1 id="2-信息提取之Beautiful-Soup库入门"><a href="#2-信息提取之Beautiful-Soup库入门" class="headerlink" title="2.信息提取之Beautiful Soup库入门"></a>2.信息提取之Beautiful Soup库入门</h1><h2 id="Beautiful-Soup库安装"><a href="#Beautiful-Soup库安装" class="headerlink" title="Beautiful Soup库安装"></a>Beautiful Soup库安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">"http://python123.io/ws/demo.html"</span>)</span><br><span class="line">demo = r.text</span><br><span class="line">form bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment">#从bs4中引入BeautifulSoup类</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br></pre></td></tr></table></figure>

<p>Beautiful Soup库是解析、遍历、维护“标签树”的功能库</p>
<h2 id="Beautiful-Soup库的基本元素"><a href="#Beautiful-Soup库的基本元素" class="headerlink" title="Beautiful Soup库的基本元素"></a>Beautiful Soup库的基本元素</h2><h3 id="Beautiful-Soup库的引用"><a href="#Beautiful-Soup库的引用" class="headerlink" title="Beautiful Soup库的引用"></a>Beautiful Soup库的引用</h3><p>Beautiful Soup库，也叫beautifulsoup4或bs4.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(demo,<span class="string">"html.parser"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Beautiful-Soup类的基本元素"><a href="#Beautiful-Soup类的基本元素" class="headerlink" title="Beautiful Soup类的基本元素"></a>Beautiful Soup类的基本元素</h3><table>
<thead>
<tr>
<th>基本元素</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Tag</td>
<td>标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾</td>
</tr>
<tr>
<td>Name</td>
<td>标签的名字，<p>…</p>的名字是’p’，格式：<tag>.name</tag></td>
</tr>
<tr>
<td>Attributes</td>
<td>标签的属性，字典形式组织，格式：<tag>.attrs</tag></td>
</tr>
<tr>
<td>NavigableString</td>
<td>标签内非属性字符串，&lt;&gt;…&lt;/&gt;中字符串，格式：<tag>.string</tag></td>
</tr>
<tr>
<td>Comment</td>
<td>标签内字符串的注释部分，一种特殊的Comment类型</td>
</tr>
</tbody></table>
<h2 id="基于bs4库的HTML内容遍历方法"><a href="#基于bs4库的HTML内容遍历方法" class="headerlink" title="基于bs4库的HTML内容遍历方法"></a>基于bs4库的HTML内容遍历方法</h2><h3 id="下行遍历"><a href="#下行遍历" class="headerlink" title="下行遍历"></a>下行遍历</h3><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.contents(列表类型)</td>
<td>子节点的列表，将<tag>所有儿子节点存入列表</tag></td>
</tr>
<tr>
<td>.children</td>
<td>子节点的迭代类型，与.contents类似，用于循环遍历儿子节点</td>
</tr>
<tr>
<td>.descendants</td>
<td>子孙节点的迭代类型，包含所有子孙节点，用于循环遍历</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#遍历儿子节点</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children</span><br><span class="line">	print(child)</span><br><span class="line"><span class="comment">#遍历子孙节点</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.descendants</span><br><span class="line">	print(child)</span><br></pre></td></tr></table></figure>

<h3 id="上行遍历"><a href="#上行遍历" class="headerlink" title="上行遍历"></a>上行遍历</h3><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.parent</td>
<td>节点的父亲标签</td>
</tr>
<tr>
<td>.parents</td>
<td>节点先辈标签的迭代类型，用于循环遍历先辈节点</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(demo,<span class="string">"html.parser"</span>)</span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">    <span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        print(parent)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(parent.name)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="comment">#p</span></span><br><span class="line"><span class="comment">#body</span></span><br><span class="line"><span class="comment">#html</span></span><br><span class="line"><span class="comment">#[document]</span></span><br></pre></td></tr></table></figure>

<h3 id="平行遍历"><a href="#平行遍历" class="headerlink" title="平行遍历"></a>平行遍历</h3><p>平行遍历发生在同一个父节点下的各节点间。</p>
<p>下一个获取的可能是字符串类型，不一定是下一个节点。</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.next_sibling</td>
<td>返回按照HTML文本顺序的下一个平行节点标签</td>
</tr>
<tr>
<td>.previous_sibling</td>
<td>返回按照HTML文本顺序的上一个平行节点标签</td>
</tr>
<tr>
<td>.next_siblings</td>
<td>迭代类型，返回按照HTML文本顺序的后续所有平行节点标签</td>
</tr>
<tr>
<td>.previous_siblings</td>
<td>迭代类型，返回按照HTML文本顺序的前续所有平行节点标签</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#遍历后续节点</span></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_siblings</span><br><span class="line">	print(sibling)</span><br><span class="line"><span class="comment">#遍历前续节点</span></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_siblings</span><br><span class="line">	print(sibling)</span><br></pre></td></tr></table></figure>

<h2 id="基于bs4库的HTML格式化和编码"><a href="#基于bs4库的HTML格式化和编码" class="headerlink" title="基于bs4库的HTML格式化和编码"></a>基于bs4库的HTML格式化和编码</h2><h3 id="格式化方法：-prettify"><a href="#格式化方法：-prettify" class="headerlink" title="格式化方法：.prettify()"></a>格式化方法：.prettify()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(demo,<span class="string">"html.parser"</span>)</span><br><span class="line">print(soup.a.prettify())</span><br></pre></td></tr></table></figure>

<h3 id="编码：默认utf-8"><a href="#编码：默认utf-8" class="headerlink" title="编码：默认utf-8"></a>编码：默认utf-8</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(<span class="string">"&lt;p&gt;中文&lt;/p&gt;"</span>,<span class="string">"html.parser"</span>)</span><br><span class="line">soup.p.string</span><br><span class="line"><span class="comment">#'中文'</span></span><br><span class="line">print(soup.p.prettify())</span><br><span class="line"><span class="comment">#&lt;p&gt;</span></span><br><span class="line"><span class="comment">#  中文</span></span><br><span class="line"><span class="comment">#&lt;/p&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="3-信息组织与提取"><a href="#3-信息组织与提取" class="headerlink" title="3.信息组织与提取"></a>3.信息组织与提取</h1><h2 id="信息标记的三种形式"><a href="#信息标记的三种形式" class="headerlink" title="信息标记的三种形式"></a>信息标记的三种形式</h2><p>标记后的信息可形成信息组织结构，增加了信息的维度；</p>
<p>标记后的信息可用于通信、存储和展示；</p>
<p>标记的结构和信息一样具有重要价值；</p>
<p>标记后的信息有利于程序的理解和运用。</p>
<h3 id="XML-eXtensible-Matkup-Language"><a href="#XML-eXtensible-Matkup-Language" class="headerlink" title="XML: eXtensible Matkup Language"></a>XML: eXtensible Matkup Language</h3><p>最早的通用信息标记语言，可扩展性好，但繁琐。</p>
<p>用于Internet上的信息交互和传递。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>...<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>/&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="JSON-JavaScript-Object-Notation"><a href="#JSON-JavaScript-Object-Notation" class="headerlink" title="JSON:  JavaScript Object Notation"></a>JSON:  JavaScript Object Notation</h3><p>信息有类型，适合程序处理(js)，较XML简洁。</p>
<p>用于移动应用云端和节点的信息通信，无注释。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#有类型的键值对表示信息的标记形式</span><br><span class="line">"key":"value"</span><br><span class="line">"key":["value1","value2"]</span><br><span class="line">"key":&#123;"subkey":"subvalue"&#125;</span><br></pre></td></tr></table></figure>

<h3 id="YAMl-YAML-Ain’t-Markup-Language"><a href="#YAMl-YAML-Ain’t-Markup-Language" class="headerlink" title="YAMl: YAML Ain’t Markup Language"></a>YAMl: YAML Ain’t Markup Language</h3><p>信息无类型，文本信息比例最高，可读性好。</p>
<p>用于各类系统的配置文件，有注释易读。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#无类型的键值对表示信息的标记形式</span></span><br><span class="line"><span class="string">key</span> <span class="string">:</span> <span class="string">"value"</span></span><br><span class="line"><span class="string">key</span> <span class="string">:</span> <span class="comment">#comment</span></span><br><span class="line"><span class="bullet">-</span><span class="string">value1</span></span><br><span class="line"><span class="bullet">-</span><span class="string">value2</span></span><br><span class="line"><span class="string">key</span> <span class="string">:</span></span><br><span class="line">	<span class="string">subkey</span> <span class="string">:</span> <span class="string">subvalue</span></span><br></pre></td></tr></table></figure>

<h2 id="信息提取的一般方法"><a href="#信息提取的一般方法" class="headerlink" title="信息提取的一般方法"></a>信息提取的一般方法</h2><h3 id="方法一：完整解析信息的标记形式，再提取关键信息。"><a href="#方法一：完整解析信息的标记形式，再提取关键信息。" class="headerlink" title="方法一：完整解析信息的标记形式，再提取关键信息。"></a>方法一：完整解析信息的标记形式，再提取关键信息。</h3><p>XML JSON YAML</p>
<p>需要标记解析器，例如bs4库的标签树遍历。</p>
<p>优点：信息解析准确</p>
<p>缺点：提取过程繁琐，过程慢</p>
<h3 id="方法二：无视标记形式，直接搜索关键信息"><a href="#方法二：无视标记形式，直接搜索关键信息" class="headerlink" title="方法二：无视标记形式，直接搜索关键信息"></a>方法二：无视标记形式，直接搜索关键信息</h3><p>搜索</p>
<p>对信息的文本查找函数即可。</p>
<p>优点：提取过程简洁，速度较快</p>
<p>缺点：提取过程准确性与信息内容相关</p>
<h3 id="融合方法：结合形式解析与搜索方法-提取关键信息"><a href="#融合方法：结合形式解析与搜索方法-提取关键信息" class="headerlink" title="融合方法：结合形式解析与搜索方法,提取关键信息"></a>融合方法：结合形式解析与搜索方法,提取关键信息</h3><p>XML JSON YAML  搜索</p>
<p>需要标记解析器及文本查找函数。</p>
<h3 id="实例：提取HTML中所有URL链接"><a href="#实例：提取HTML中所有URL链接" class="headerlink" title="实例：提取HTML中所有URL链接"></a>实例：提取HTML中所有URL链接</h3><blockquote>
<p>思路：    1. 搜索到所有<a>标签</a></p>
<p>​        2.解析<a>标签格式，提取href后的链接内容</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">form bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(demo,<span class="string">"html.parser"</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</span><br><span class="line">	print(link.get(<span class="string">'href'</span>))</span><br></pre></td></tr></table></figure>

<h2 id="基于bs4库的HTML内容查找方法"><a href="#基于bs4库的HTML内容查找方法" class="headerlink" title="基于bs4库的HTML内容查找方法"></a>基于bs4库的HTML内容查找方法</h2><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;&gt;.find_all(name,attrs,recursive,string,**kwargs)</td>
<td>返回一个列表类型，存储查找的结果</td>
</tr>
</tbody></table>
<blockquote>
<p>简写形式：<tag>(..) 等价于 <tag>.find_all(..)</tag></tag></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#name:对标签名称的检索字符串</span></span><br><span class="line">soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">soup.find_all([<span class="string">'a'</span>, <span class="string">'b'</span>])</span><br><span class="line">soup.find_all(<span class="literal">True</span>) <span class="comment">#返回soup的所有标签信息</span></span><br><span class="line"><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(<span class="literal">True</span>):</span><br><span class="line">    print(tag.name) <span class="comment">#html head title body p b p a a</span></span><br><span class="line"><span class="comment">#输出所有b开头的标签，包括b和body    </span></span><br><span class="line"><span class="comment">#引入正则表达式库</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(re.compile(<span class="string">'b'</span>)):</span><br><span class="line">    print(tag.name) <span class="comment">#body b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#attrs:对标签属性值的检索字符串，可标注属性检索</span></span><br><span class="line">soup.find_all(<span class="string">'p'</span>, <span class="string">'course'</span>)</span><br><span class="line">soup.find_all(id=<span class="string">'link1'</span>)</span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line">soup.find_all(id=re.compile(<span class="string">'link'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#recursive:是否对子孙全部检索，默认为True</span></span><br><span class="line">soup.find_all(<span class="string">'p'</span>, recursive = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#string:&lt;&gt;...&lt;/&gt;字符串区域的检索字符串</span></span><br><span class="line">soup.find_all(string = <span class="string">"Basic Python"</span>)</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">soup.find_all(string = re.compile(<span class="string">'Python'</span>))</span><br><span class="line"><span class="comment">#简写形式：soup(..) = soup.find_all(..)</span></span><br></pre></td></tr></table></figure>

<p>拓展方法：参数同.find_all()</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;&gt;.find()</td>
<td>搜索且只返回一个结果，字符串类型</td>
</tr>
<tr>
<td>&lt;&gt;.find_parents()</td>
<td>在先辈节点中搜索，返回列表类型</td>
</tr>
<tr>
<td>&lt;&gt;.find_parent()</td>
<td>在先辈节点中返回一个结果，字符串类型</td>
</tr>
<tr>
<td>&lt;&gt;.find_next_siblings()</td>
<td>在后续平行节点中搜索，返回列表类型</td>
</tr>
<tr>
<td>&lt;&gt;.find_next_sibling()</td>
<td>在后续平行节点中返回一个结果，字符串类型</td>
</tr>
<tr>
<td>&lt;&gt;.find_previous_siblings()</td>
<td>在前续平行节点中搜索，返回列表类型</td>
</tr>
<tr>
<td>&lt;&gt;.find_previous_sibling()</td>
<td>在前续平行节点中返回一个结果，字符串类型</td>
</tr>
</tbody></table>
<h1 id="4-信息提取实例"><a href="#4-信息提取实例" class="headerlink" title="4.信息提取实例"></a>4.信息提取实例</h1><h2 id="中国大学排名定向爬虫"><a href="#中国大学排名定向爬虫" class="headerlink" title="中国大学排名定向爬虫"></a>中国大学排名定向爬虫</h2><blockquote>
<p>功能描述：</p>
<p>​    输入：大学排名URL链接</p>
<p>​    输出：大学排名信息的屏幕输出（排名，大学名称，总分）</p>
<p>​    技术路线：requests-bs4</p>
<p>​    定向爬虫：仅对输入URL进行爬取，不拓展爬取</p>
</blockquote>
<blockquote>
<p>程序的结构设计：</p>
<p>​    步骤1：从网络上获取大学排名网页内容 </p>
<p>​            getHTMLText()</p>
<p>​    步骤2：提取网页内容中信息到合适的数据结构</p>
<p>​            fillUnivList()</p>
<p>​    步骤3：利用数据结构展示并输出结果</p>
<p>​            printUnivList()</p>
</blockquote>
<h2 id="初步代码编写"><a href="#初步代码编写" class="headerlink" title="初步代码编写"></a>初步代码编写</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout= <span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillUnivList</span><span class="params">(ulist, html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">"html.parser"</span>)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">'tbody'</span>).children:</span><br><span class="line">        <span class="keyword">if</span> isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds = tr(<span class="string">'td'</span>)</span><br><span class="line">            ulist.append([tds[<span class="number">0</span>].string, tds[<span class="number">1</span>].string, tds[<span class="number">3</span>].string])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span></span><br><span class="line">    print(<span class="string">"&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;"</span>.format(<span class="string">"排名"</span>, <span class="string">"学校名称"</span>, <span class="string">"分数"</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(<span class="string">"&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;"</span>.format(u[<span class="number">0</span>], u[<span class="number">1</span>], u[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    uinfo = []</span><br><span class="line">    url = <span class="string">'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'</span></span><br><span class="line">    html = getHTMLText(url)</span><br><span class="line">    fillUnivList(uinfo,html)</span><br><span class="line">    printUnivList(uinfo,<span class="number">20</span>) <span class="comment">#20 univs</span></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h2 id="中文输出对齐问题"><a href="#中文输出对齐问题" class="headerlink" title="中文输出对齐问题"></a>中文输出对齐问题</h2><p>当输出中文的宽度不够时，系统会采用西文字符填充，导致对齐出现问题。</p>
<p>可以使用中文空格chr(12288)填充解决。</p>
<p><code>&lt;填充&gt;</code>：用于填充的单个字符</p>
<p><code>&lt;对齐&gt;</code>：&lt;左对齐    &gt;右对齐        ^居中对齐</p>
<p><code>&lt;宽度&gt;</code>：槽的设定输出宽度</p>
<p><code>,</code>：数字的千位分隔符适用于整数和浮点数</p>
<p><code>&lt;精度&gt;</code>：浮点数小数部分的精度或字符串的最大输出长度</p>
<p><code>&lt;类型&gt;</code>：整数类型b,c,d,o,x,X浮点数类型e,E,f,%</p>
<h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout= <span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillUnivList</span><span class="params">(ulist, html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">"html.parser"</span>)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">'tbody'</span>).children:</span><br><span class="line">        <span class="keyword">if</span> isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds = tr(<span class="string">'td'</span>)</span><br><span class="line">            ulist.append([tds[<span class="number">0</span>].string, tds[<span class="number">1</span>].string, tds[<span class="number">3</span>].string])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span></span><br><span class="line">    tplt = <span class="string">"&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;"</span></span><br><span class="line">    print(tplt.format(<span class="string">"排名"</span>, <span class="string">"学校名称"</span>, <span class="string">"分数"</span>,chr(<span class="number">12288</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(tplt.format(u[<span class="number">0</span>], u[<span class="number">1</span>], u[<span class="number">2</span>],chr(<span class="number">12288</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    uinfo = []</span><br><span class="line">    url = <span class="string">'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'</span></span><br><span class="line">    html = getHTMLText(url)</span><br><span class="line">    fillUnivList(uinfo,html)</span><br><span class="line">    printUnivList(uinfo,<span class="number">20</span>) <span class="comment">#20 univs</span></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h1 id="5-实战之Re库入门"><a href="#5-实战之Re库入门" class="headerlink" title="5.实战之Re库入门"></a>5.实战之Re库入门</h1><p>正则表达式</p>
<ul>
<li>通用的字符串表达框架</li>
<li>简介表达一组字符串的表达式</li>
<li>针对字符串表达“简洁”和“特征”思想的工具</li>
<li>判断某字符串的特征归属</li>
</ul>
<h2 id="正则表达式的语法"><a href="#正则表达式的语法" class="headerlink" title="正则表达式的语法"></a>正则表达式的语法</h2><table>
<thead>
<tr>
<th>操作符</th>
<th>说明</th>
<th>实例</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>表示任何单个字符</td>
<td></td>
</tr>
<tr>
<td>[ ]</td>
<td>字符集，对单个字符给出取值范围</td>
<td>[abc]表达式a、b、c,[a-z]表示a到z单个字符</td>
</tr>
<tr>
<td>[^ ]</td>
<td>非字符集，对单个字符给出排除范围</td>
<td>[^abc]表示非a或b或c的单个字符</td>
</tr>
<tr>
<td>*</td>
<td>前一个字符0次或无限次扩展</td>
<td>abc* 表示 ab、abc、abcc、abccc等</td>
</tr>
<tr>
<td>+</td>
<td>前一个字符1次或无限次扩展</td>
<td>abc+ 表示 abc、abcc、abccc等</td>
</tr>
<tr>
<td>?</td>
<td>前一个字符0次或1次扩展</td>
<td>abc？表示 ab、abc</td>
</tr>
<tr>
<td>|</td>
<td>左右表达式任意一个</td>
<td>abc|def 表示 abc 、def</td>
</tr>
<tr>
<td>{m}</td>
<td>扩展前一个字符m次</td>
<td>ab{2}c表示abbc</td>
</tr>
<tr>
<td>{m,n}</td>
<td>扩展前一个字符m至n次（含n）</td>
<td>ab{1,2}c表示abc、abbc</td>
</tr>
<tr>
<td>^</td>
<td>匹配字符串开头</td>
<td>^abc表示abc且在一个字符串的开头</td>
</tr>
<tr>
<td>$</td>
<td>匹配字符串结尾</td>
<td>abc$表示abc且在一个字符串的结尾</td>
</tr>
<tr>
<td>( )</td>
<td>分组标记，内部只能使用|操作符</td>
<td>(abc)表示abc，{abc|def}表示abc、def</td>
</tr>
<tr>
<td>\d</td>
<td>数字，等价于[0-9]</td>
<td></td>
</tr>
<tr>
<td>\w</td>
<td>单词字符，等价于[A-Za-z0-9_]</td>
<td></td>
</tr>
</tbody></table>
<h3 id="经典正则表达式实例"><a href="#经典正则表达式实例" class="headerlink" title="经典正则表达式实例"></a>经典正则表达式实例</h3><table>
<thead>
<tr>
<th>正则表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>^[A-Za-z]+$</code></td>
<td>由26个字母组成的字符串</td>
</tr>
<tr>
<td><code>^[A-Za-z0-9]+$</code></td>
<td>由26个字母和数字组成的字符串</td>
</tr>
<tr>
<td><code>^-?\d+$</code></td>
<td>整数形式的字符串</td>
</tr>
<tr>
<td><code>^[0-9]*[1-9][0-9]*$</code></td>
<td>正整数形式的字符串</td>
</tr>
<tr>
<td><code>[1-9]\d{5}</code></td>
<td>中国境内邮政编码，6位</td>
</tr>
<tr>
<td><code>[\u4e00-\u9fa5]</code></td>
<td>匹配中文字符</td>
</tr>
<tr>
<td>`\d{3}-\d{8}</td>
<td>\d{4}-\d{7}`</td>
</tr>
</tbody></table>
<h2 id="Re库的基本使用"><a href="#Re库的基本使用" class="headerlink" title="Re库的基本使用"></a>Re库的基本使用</h2><p>Re库是Python的标准库，主要用于字符串匹配。</p>
<h3 id="正则表达式的表示类型"><a href="#正则表达式的表示类型" class="headerlink" title="正则表达式的表示类型"></a>正则表达式的表示类型</h3><p>raw string类型（原生字符串类型）,是不包含转义符<code>\</code>的字符串</p>
<p>re库采用raw string类型表示正则表达式，表示为：r’text’</p>
<p>例如：<code>r&#39;[1-9]\d{5}&#39;</code></p>
<p>​       <code>r&#39;\d{3}-\d{8}|\d{4}-\d{7}&#39;</code></p>
<h3 id="Re库主要功能函数"><a href="#Re库主要功能函数" class="headerlink" title="Re库主要功能函数"></a>Re库主要功能函数</h3><table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>re.search()</td>
<td>在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</td>
</tr>
<tr>
<td>re.match()</td>
<td>从一个字符串的开始位置起匹配正则表达式，返回match对象</td>
</tr>
<tr>
<td>re.findall()</td>
<td>搜索字符串，以列表类型返回全部能匹配的子串</td>
</tr>
<tr>
<td>re.split()</td>
<td>将一个字符串按照正则表达式匹配结果进行分割，返回列表类型</td>
</tr>
<tr>
<td>re.finditer()</td>
<td>搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象</td>
</tr>
<tr>
<td>re.sub()</td>
<td>在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td>
</tr>
</tbody></table>
<h4 id="re-search-pattern-string-flags-0"><a href="#re-search-pattern-string-flags-0" class="headerlink" title="re.search(pattern,string,flags=0)"></a>re.search(pattern,string,flags=0)</h4><blockquote>
<p>re.search(pattern,string,flags=0)</p>
</blockquote>
<ul>
<li><p>在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象；</p>
<ul>
<li><p>pattern：正则表达式的字符串或原生字符串表示；</p>
</li>
<li><p>string：待匹配字符串；</p>
</li>
<li><p>flags：正则表达式使用时的控制标记；</p>
<table>
<thead>
<tr>
<th>常用标记</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>re.I|re.IGNORECASE</td>
<td>忽略正则表达式的大小写，[A-Z]能匹配小写字符</td>
</tr>
<tr>
<td>re.M|re.MUTILINE</td>
<td>正则表达式中的^操作符能够将给定字符串的每行当做匹配开始</td>
</tr>
<tr>
<td>re.S|re.DOTILL</td>
<td>正则表达式中的.操作符能够匹配所有字符，默认匹配除换行符外的所有字符</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT 100081'</span>)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.group(<span class="number">0</span>))  <span class="comment">#'100081'</span></span><br></pre></td></tr></table></figure>

<h4 id="re-match-pattern-string-flags-0"><a href="#re-match-pattern-string-flags-0" class="headerlink" title="re.match(pattern,string,flags=0)"></a>re.match(pattern,string,flags=0)</h4><blockquote>
<p>re.match(pattern,string,flags=0)</p>
</blockquote>
<ul>
<li>从一个字符串的开始位置起匹配正则表达式，返回match对象<ul>
<li>pattern：正则表达式的字符串或原生字符串表示；</li>
<li>string：待匹配字符串；</li>
<li>flags：正则表达式使用时的控制标记；</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.match(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT 100081'</span>)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.group(<span class="number">0</span>))  <span class="comment">#NULL</span></span><br><span class="line">match = re.match(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'100081 BIT'</span>)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.group(<span class="number">0</span>))  <span class="comment">#'100081'</span></span><br></pre></td></tr></table></figure>

<h4 id="re-findall-pattern-string-flags-0"><a href="#re-findall-pattern-string-flags-0" class="headerlink" title="re.findall(pattern,string,flags=0)"></a>re.findall(pattern,string,flags=0)</h4><blockquote>
<p>re.findall(pattern,string,flags=0)</p>
</blockquote>
<ul>
<li>搜索字符串，以列表类型返回全部能匹配的子串<ul>
<li>pattern：正则表达式的字符串或原生字符串表示；</li>
<li>string：待匹配字符串；</li>
<li>flags：正则表达式使用时的控制标记；</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">ls = re.findall(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line">print(ls) <span class="comment">#['100081', '100084']</span></span><br></pre></td></tr></table></figure>

<h4 id="re-split-pattern-string-maxsplit-0-flags-0"><a href="#re-split-pattern-string-maxsplit-0-flags-0" class="headerlink" title="re.split(pattern,string,maxsplit=0,flags=0)"></a>re.split(pattern,string,maxsplit=0,flags=0)</h4><blockquote>
<p>re.split(pattern,string,flags=0)</p>
</blockquote>
<ul>
<li>将一个字符串按照正则表达式匹配结果进行分割，返回列表类型<ul>
<li>pattern：正则表达式的字符串或原生字符串表示；</li>
<li>string：待匹配字符串；</li>
<li>maxsplit：最大分割数，剩余部分作为最后一个元素输出；</li>
<li>flags：正则表达式使用时的控制标记；</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">ls = re.split(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line">print(ls) <span class="comment">#['BIT', ' TSU', '']</span></span><br><span class="line">ls2 = re.split(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>, maxsplit=<span class="number">1</span>)</span><br><span class="line">print(ls2) <span class="comment">#['BIT', ' TSU10084']</span></span><br></pre></td></tr></table></figure>

<h4 id="re-finditer-pattern-string-flags-0"><a href="#re-finditer-pattern-string-flags-0" class="headerlink" title="re.finditer(pattern,string,flags=0)"></a>re.finditer(pattern,string,flags=0)</h4><blockquote>
<p>re.finditer(pattern,string,flags=0)</p>
</blockquote>
<ul>
<li>搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素都是match对象<ul>
<li>pattern：正则表达式的字符串或原生字符串表示；</li>
<li>string：待匹配字符串；</li>
<li>flags：正则表达式使用时的控制标记；</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>):</span><br><span class="line">    <span class="keyword">if</span> m:</span><br><span class="line">        print(m.group(<span class="number">0</span>)) <span class="comment">#100081 100084</span></span><br></pre></td></tr></table></figure>

<h4 id="re-sub-pattern-repl-string-count-0-flags-0"><a href="#re-sub-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.sub(pattern,repl,string,count=0,flags=0)"></a>re.sub(pattern,repl,string,count=0,flags=0)</h4><blockquote>
<p>re.sub(pattern,repl,string,count=0,flags=0)</p>
</blockquote>
<ul>
<li>在一个字符串中替换所有匹配正则表达式的子串，并返回替换后的字符串<ul>
<li>pattern：正则表达式的字符串或原生字符串表示；</li>
<li>repl：替换匹配字符串的字符串；</li>
<li>string：待匹配字符串；</li>
<li>count：匹配的最大替换次数</li>
<li>flags：正则表达式使用时的控制标记；</li>
</ul>
</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">rst = re.sub(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">':zipcode'</span>, <span class="string">'BIT 100081,TSU 100084'</span>)</span><br><span class="line">print(rst) <span class="comment"># 'BIT :zipcode TSU :zipcode'</span></span><br></pre></td></tr></table></figure>

<h3 id="Re库的另一种用法"><a href="#Re库的另一种用法" class="headerlink" title="Re库的另一种用法"></a>Re库的另一种用法</h3><p>编译后的对象拥有的方法和re库主要功能函数相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#函数式用法：一次性操作</span></span><br><span class="line">rst = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT 100081'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#面向对象用法：编译后的多次操作</span></span><br><span class="line">pat = re.compile(<span class="string">r'[1-9]\d&#123;5&#125;'</span>)</span><br><span class="line">rst = pat.search(<span class="string">'BIT 100081'</span>)</span><br></pre></td></tr></table></figure>

<h4 id="re-compile-pattern-flags-0"><a href="#re-compile-pattern-flags-0" class="headerlink" title="re.compile(pattern,flags=0)"></a>re.compile(pattern,flags=0)</h4><ul>
<li>将正则表达式的字符串形式编译成正则表达式对象<ul>
<li>pattern：正则表达式的字符串或原生字符串表示；</li>
<li>flags：正则表达式使用时的控制标记；</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regex = re.compile(<span class="string">r'[1-9]\d&#123;5&#125;'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Re库的match对象"><a href="#Re库的match对象" class="headerlink" title="Re库的match对象"></a>Re库的match对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT 100081'</span>)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    print(match.group(<span class="number">0</span>))  <span class="comment"># '100081'</span></span><br><span class="line">print(type(match)) <span class="comment"># &lt;class 're.Match'&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="Match对象的属性"><a href="#Match对象的属性" class="headerlink" title="Match对象的属性"></a>Match对象的属性</h4><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.string</td>
<td>待匹配的文本</td>
</tr>
<tr>
<td>.re</td>
<td>匹配时使用的pattern对象（正则表达式）</td>
</tr>
<tr>
<td>.pos</td>
<td>正则表达式搜索文本的开始位置</td>
</tr>
<tr>
<td>.endpos</td>
<td>正则表达式搜索文本的结束位置</td>
</tr>
</tbody></table>
<h4 id="Match对象的方法"><a href="#Match对象的方法" class="headerlink" title="Match对象的方法"></a>Match对象的方法</h4><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.group(0)</td>
<td>获得匹配后的字符串</td>
</tr>
<tr>
<td>.start()</td>
<td>匹配字符串在原始字符串的开始位置</td>
</tr>
<tr>
<td>.end()</td>
<td>匹配字符串在原始字符串的结束位置</td>
</tr>
<tr>
<td>.span()</td>
<td>返回(.start(),.end())</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">m = re.search(<span class="string">r'[1-9]\d&#123;5&#125;'</span>, <span class="string">'BIT100081 TSU100084'</span>)</span><br><span class="line">print(m.string) <span class="comment"># BIT100081 TSU100084</span></span><br><span class="line">print(m.re) <span class="comment"># re.compile('[1-9]\\d&#123;5&#125;')</span></span><br><span class="line">print(m.pos) <span class="comment"># 0</span></span><br><span class="line">print(m.endpos) <span class="comment"># 19</span></span><br><span class="line">print(m.group(<span class="number">0</span>)) <span class="comment"># '100081' 返回的是第一次匹配的结果,获取所有使用re.finditer()方法</span></span><br><span class="line">print(m.start()) <span class="comment"># 3</span></span><br><span class="line">print(m.end()) <span class="comment"># 9</span></span><br><span class="line">print(m.span()) <span class="comment"># (3, 9)</span></span><br></pre></td></tr></table></figure>

<h3 id="Re库的贪婪匹配和最小匹配"><a href="#Re库的贪婪匹配和最小匹配" class="headerlink" title="Re库的贪婪匹配和最小匹配"></a>Re库的贪婪匹配和最小匹配</h3><p>Re库默认采用贪婪匹配，即输出匹配最长的子串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.search(<span class="string">r'PY.*N'</span>, <span class="string">'PYANBNCNDN'</span>)</span><br><span class="line">print(match.group(<span class="number">0</span>)) <span class="comment"># PYANBNCNDN</span></span><br></pre></td></tr></table></figure>

<p>最小匹配方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.search(<span class="string">r'PY.*?N'</span>, <span class="string">'PYANBNCNDN'</span>)</span><br><span class="line">print(match.group(<span class="number">0</span>)) <span class="comment"># PYAN</span></span><br></pre></td></tr></table></figure>

<p>最小匹配操作符</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>*?</td>
<td>前一个字符0次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td>+?</td>
<td>前一个字符1次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td>??</td>
<td>前一个字符0次或1次扩展，最小匹配</td>
</tr>
<tr>
<td>{m,n}?</td>
<td>扩展前一个字符m至n次（含n），最小匹配</td>
</tr>
</tbody></table>
<h2 id="Re库实例之淘宝商品比价定向爬虫"><a href="#Re库实例之淘宝商品比价定向爬虫" class="headerlink" title="Re库实例之淘宝商品比价定向爬虫"></a>Re库实例之淘宝商品比价定向爬虫</h2><blockquote>
<p>功能描述：</p>
<ul>
<li>目标：获取淘宝搜索页面的信息，提取其中的商品名称和价格</li>
<li>理解：<ul>
<li>淘宝的搜索接口</li>
<li>翻页的处理</li>
</ul>
</li>
<li>技术路线：requests-re</li>
</ul>
</blockquote>
<blockquote>
<p>程序的结构设计：</p>
<ul>
<li>步骤1：提交商品搜索请求，循环获取页面</li>
<li>步骤2：对于每个页面，提取商品的名称和价格信息</li>
<li>步骤3：将信息输出到屏幕上</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment">#浏览器请求头中的User-Agent，代表当前请求的用户代理信息（下方有获取方式）</span></span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'</span>&#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#浏览器请求头中的cookie，包含自己账号的登录信息（下方有获取方式）</span></span><br><span class="line">        coo = <span class="string">''</span></span><br><span class="line">        cookies = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> coo.split(<span class="string">';'</span>): <span class="comment">#浏览器伪装</span></span><br><span class="line">            name, value = line.strip().split(<span class="string">'='</span>, <span class="number">1</span>)</span><br><span class="line">            cookies[name] = value</span><br><span class="line">        r = requests.get(url, cookies = cookies, headers=headers, timeout = <span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#解析请求到的页面，提取出相关商品的价格和名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePage</span><span class="params">(ilt, html)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        plt = re.findall(<span class="string">r'\"view_price\"\:\"[\d\.]*\"'</span>, html)</span><br><span class="line">        tlt = re.findall(<span class="string">r'\"raw_title\"\:\".*?\"'</span>, html)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(plt)):</span><br><span class="line">            price = eval(plt[i].split(<span class="string">':'</span>)[<span class="number">1</span>])</span><br><span class="line">            title = eval(tlt[i].split(<span class="string">':'</span>)[<span class="number">1</span>])</span><br><span class="line">            ilt.append([price, title])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printGoodsList</span><span class="params">(ilt)</span>:</span></span><br><span class="line">    tplt = <span class="string">"&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;"</span></span><br><span class="line">    print(tplt.format(<span class="string">"序号"</span>, <span class="string">"价格"</span>, <span class="string">"商品名称"</span>))</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> g <span class="keyword">in</span> ilt:</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        print(tplt.format(count, g[<span class="number">0</span>], g[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    goods = <span class="string">'书包'</span></span><br><span class="line">    depth = <span class="number">2</span> <span class="comment">#爬取深度，2表示爬取两页数据</span></span><br><span class="line">    start_url = <span class="string">'https://s.taobao.com/search?q='</span> + goods</span><br><span class="line">    infoList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(depth):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = start_url + <span class="string">'&amp;s='</span> + str(<span class="number">44</span>*i)</span><br><span class="line">            html = getHTMLText(url)</span><br><span class="line">            parsePage(infoList, html)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    printGoodsList(infoList)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p>​    需要注意的是，淘宝网站本身有反爬虫机制，所以在使用<code>requests</code>库的<code>get()</code>方法爬取网页信息时，需要加入本地的cookie信息，否则淘宝返回的是一个错误页面，无法获取数据。</p>
<p>​    代码中的<code>coo</code>变量中需要自己添加浏览器中的<code>cookie</code>信息，具体做法是在浏览器中按F12，在出现的窗口中进入<code>network</code>（网络）内，搜索“书包”，然后找到请求的url（一般是第一个），点击请求在右侧<code>header</code>（消息头）中找到<code>Request Header</code>（请求头），在请求头中找到<code>User-Agent</code>和<code>cookie</code>字段，放到代码相应位置即可。</p>
<h2 id="Re库实例之股票数据定向爬虫"><a href="#Re库实例之股票数据定向爬虫" class="headerlink" title="Re库实例之股票数据定向爬虫"></a>Re库实例之股票数据定向爬虫</h2><blockquote>
<p>功能描述：</p>
<ul>
<li>目标：获取上交所和深交所所有股票的名称和交易信息</li>
<li>输出：保存到文件中</li>
<li>技术路线：requests-bs4-re</li>
</ul>
</blockquote>
<blockquote>
<p>候选数据网站的选择：</p>
<ul>
<li>新浪股票：<a href="https://finance.sina.com.cn/stock/" target="_blank" rel="noopener">https://finance.sina.com.cn/stock/</a></li>
<li>百度股票：<a href="https://gupiao.baidu.com/stock/" target="_blank" rel="noopener">https://gupiao.baidu.com/stock/</a></li>
<li>选取原则：股票信息静态存在于HTML页面中，非js代码生成，没有Robots协议限制。</li>
</ul>
</blockquote>
<blockquote>
<p>程序的结构设计</p>
<ul>
<li>步骤1：从东方财富网获取股票列表</li>
<li>步骤2：根据股票列表逐个到百度股票获取个股信息</li>
<li>步骤3：将结果存储到文件</li>
</ul>
</blockquote>
<h3 id="初步代码编写-error"><a href="#初步代码编写-error" class="headerlink" title="初步代码编写(error)"></a>初步代码编写(error)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockList</span><span class="params">(lst, stockURL)</span>:</span></span><br><span class="line">    html = getHTMLText(stockURL)</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'html.parser'</span>) </span><br><span class="line">    a = soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            href = i.attrs[<span class="string">'href'</span>]</span><br><span class="line">            lst.append(re.findall(<span class="string">r"[s][hz]\d&#123;6&#125;"</span>, href)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockInfo</span><span class="params">(lst, stockURL, fpath)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> stock <span class="keyword">in</span> lst:</span><br><span class="line">        url = stockURL + stock + <span class="string">".html"</span></span><br><span class="line">        html = getHTMLText(url)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> html==<span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            infoDict = &#123;&#125;</span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">            stockInfo = soup.find(<span class="string">'div'</span>,attrs=&#123;<span class="string">'class'</span>:<span class="string">'stock-bets'</span>&#125;)</span><br><span class="line"> </span><br><span class="line">            name = stockInfo.find_all(attrs=&#123;<span class="string">'class'</span>:<span class="string">'bets-name'</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">            infoDict.update(&#123;<span class="string">'股票名称'</span>: name.text.split()[<span class="number">0</span>]&#125;)</span><br><span class="line">             </span><br><span class="line">            keyList = stockInfo.find_all(<span class="string">'dt'</span>)</span><br><span class="line">            valueList = stockInfo.find_all(<span class="string">'dd'</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keyList)):</span><br><span class="line">                key = keyList[i].text</span><br><span class="line">                val = valueList[i].text</span><br><span class="line">                infoDict[key] = val</span><br><span class="line">             </span><br><span class="line">            <span class="keyword">with</span> open(fpath, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write( str(infoDict) + <span class="string">'\n'</span> )</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            traceback.print_exc()</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    stock_list_url = <span class="string">'https://quote.eastmoney.com/stocklist.html'</span></span><br><span class="line">    stock_info_url = <span class="string">'https://gupiao.baidu.com/stock/'</span></span><br><span class="line">    output_file = <span class="string">'D:/BaiduStockInfo.txt'</span></span><br><span class="line">    slist=[]</span><br><span class="line">    getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(slist, stock_info_url, output_file)</span><br><span class="line"> </span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h3 id="代码优化-error"><a href="#代码优化-error" class="headerlink" title="代码优化(error)"></a>代码优化(error)</h3><p>速度提高：编码识别的优化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url, code=<span class="string">"utf-8"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = code</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockList</span><span class="params">(lst, stockURL)</span>:</span></span><br><span class="line">    html = getHTMLText(stockURL, <span class="string">"GB2312"</span>)</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'html.parser'</span>) </span><br><span class="line">    a = soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            href = i.attrs[<span class="string">'href'</span>]</span><br><span class="line">            lst.append(re.findall(<span class="string">r"[s][hz]\d&#123;6&#125;"</span>, href)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockInfo</span><span class="params">(lst, stockURL, fpath)</span>:</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> stock <span class="keyword">in</span> lst:</span><br><span class="line">        url = stockURL + stock + <span class="string">".html"</span></span><br><span class="line">        html = getHTMLText(url)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> html==<span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            infoDict = &#123;&#125;</span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">            stockInfo = soup.find(<span class="string">'div'</span>,attrs=&#123;<span class="string">'class'</span>:<span class="string">'stock-bets'</span>&#125;)</span><br><span class="line"> </span><br><span class="line">            name = stockInfo.find_all(attrs=&#123;<span class="string">'class'</span>:<span class="string">'bets-name'</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">            infoDict.update(&#123;<span class="string">'股票名称'</span>: name.text.split()[<span class="number">0</span>]&#125;)</span><br><span class="line">             </span><br><span class="line">            keyList = stockInfo.find_all(<span class="string">'dt'</span>)</span><br><span class="line">            valueList = stockInfo.find_all(<span class="string">'dd'</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keyList)):</span><br><span class="line">                key = keyList[i].text</span><br><span class="line">                val = valueList[i].text</span><br><span class="line">                infoDict[key] = val</span><br><span class="line">             </span><br><span class="line">            <span class="keyword">with</span> open(fpath, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write( str(infoDict) + <span class="string">'\n'</span> )</span><br><span class="line">                count = count + <span class="number">1</span></span><br><span class="line">                print(<span class="string">"\r当前进度: &#123;:.2f&#125;%"</span>.format(count*<span class="number">100</span>/len(lst)),end=<span class="string">""</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            print(<span class="string">"\r当前进度: &#123;:.2f&#125;%"</span>.format(count*<span class="number">100</span>/len(lst)),end=<span class="string">""</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    stock_list_url = <span class="string">'https://quote.eastmoney.com/stocklist.html'</span></span><br><span class="line">    stock_info_url = <span class="string">'https://gupiao.baidu.com/stock/'</span></span><br><span class="line">    output_file = <span class="string">'D:/BaiduStockInfo.txt'</span></span><br><span class="line">    slist=[]</span><br><span class="line">    getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(slist, stock_info_url, output_file)</span><br><span class="line"> </span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h3 id="测试成功代码"><a href="#测试成功代码" class="headerlink" title="测试成功代码"></a>测试成功代码</h3><p>由于东方财富网链接访问时出现错误，所以更换了一个新的网站去获取股票列表，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockList</span><span class="params">(lst, stockListURL)</span>:</span></span><br><span class="line">    html = getHTMLText(stockListURL)</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">    a = soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            href = i.attrs[<span class="string">'href'</span>]</span><br><span class="line">            lst.append(re.findall(<span class="string">r"[S][HZ]\d&#123;6&#125;"</span>, href)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    lst = [item.lower() <span class="keyword">for</span> item <span class="keyword">in</span> lst]  <span class="comment"># 将爬取信息转换小写</span></span><br><span class="line">    <span class="keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockInfo</span><span class="params">(lst, stockInfoURL, fpath)</span>:</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> stock <span class="keyword">in</span> lst:</span><br><span class="line">        url = stockInfoURL + stock + <span class="string">".html"</span></span><br><span class="line">        html = getHTMLText(url)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> html == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            infoDict = &#123;&#125;</span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">            stockInfo = soup.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'stock-bets'</span>&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> isinstance(stockInfo, bs4.element.Tag):  <span class="comment"># 判断类型</span></span><br><span class="line">                name = stockInfo.find_all(attrs=&#123;<span class="string">'class'</span>: <span class="string">'bets-name'</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">                infoDict.update(&#123;<span class="string">'股票名称'</span>: name.text.split(<span class="string">'\n'</span>)[<span class="number">1</span>].replace(<span class="string">' '</span>,<span class="string">''</span>)&#125;)</span><br><span class="line">                keylist = stockInfo.find_all(<span class="string">'dt'</span>)</span><br><span class="line">                valuelist = stockInfo.find_all(<span class="string">'dd'</span>)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keylist)):</span><br><span class="line">                    key = keylist[i].text</span><br><span class="line">                    val = valuelist[i].text</span><br><span class="line">                    infoDict[key] = val</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> open(fpath, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(str(infoDict) + <span class="string">'\n'</span>)</span><br><span class="line">                count = count + <span class="number">1</span></span><br><span class="line">                print(<span class="string">"\r当前速度：&#123;:.2f&#125;%"</span>.format(count*<span class="number">100</span>/len(lst)), end=<span class="string">""</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            print(<span class="string">"\r当前速度：&#123;:.2f&#125;%"</span>.format(count*<span class="number">100</span>/len(lst)), end=<span class="string">""</span>)</span><br><span class="line">            traceback.print_exc()</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    fpath = <span class="string">'D://gupiao.txt'</span></span><br><span class="line">    stock_list_url = <span class="string">'https://hq.gucheng.com/gpdmylb.html'</span></span><br><span class="line">    stock_info_url = <span class="string">'https://gupiao.baidu.com/stock/'</span></span><br><span class="line">    slist = []</span><br><span class="line">    list = getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(list, stock_info_url, fpath)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h1 id="6-爬虫框架-Scrapy"><a href="#6-爬虫框架-Scrapy" class="headerlink" title="6.爬虫框架-Scrapy"></a>6.爬虫框架-Scrapy</h1><p>爬虫框架：是实现爬虫功能的一个软件结构和功能组件集合。</p>
<p>爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫。</p>
<p><img src="https://zsy.xyz/images/20190604/1558688318694.png" alt="1558688318694"></p>
<h2 id="安装Scrapy"><a href="#安装Scrapy" class="headerlink" title="安装Scrapy"></a>安装Scrapy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line"><span class="meta">#</span>验证</span><br><span class="line">scrapy -h</span><br></pre></td></tr></table></figure>

<h3 id="遇到错误"><a href="#遇到错误" class="headerlink" title="遇到错误"></a>遇到错误</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">building &apos;twisted.test.raiser&apos; extension</span><br><span class="line">   error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual C++ Build Tools&quot;: https://visualstudio.microsoft.com/downloads/</span><br></pre></td></tr></table></figure>

<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ol>
<li><p>查看python版本及位数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\ASUS&gt;python</span><br><span class="line">Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br></pre></td></tr></table></figure>

<p>可知，python版本为3.7.2, 64位</p>
</li>
<li><p>下载Twisted</p>
<p>到 <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a> 下载twisted对应版本的whl文件;<br>根据版本应下载Twisted‑17.9.0‑cp37‑cp37m‑win_amd64.whl</p>
<p>注意：cp后面是python版本，amd64代表64位，32位的下载32位对应的文件。</p>
</li>
<li><p>安装Twisted</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install D:\download\Twisted‑19.2.0‑cp37‑cp37m‑win_amd64.whl</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装Scrapy</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install scrapy</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Scrapy爬虫框架解析"><a href="#Scrapy爬虫框架解析" class="headerlink" title="Scrapy爬虫框架解析"></a>Scrapy爬虫框架解析</h2><ol>
<li>Engine：不需要用户修改<ul>
<li>控制所有模块之间的数据流</li>
<li>根据条件触发事件</li>
</ul>
</li>
<li>Downloader：不需要用户修改<ul>
<li>根据请求下载网页</li>
</ul>
</li>
<li>Scheduler：不需要用户修改<ul>
<li>对所有爬取请求进行调度管理</li>
</ul>
</li>
<li>Downloader Middleware：用户可编写配置代码<ul>
<li>目的：实施Engine、Scheduler和Downloader之间进行用户可配置的控制</li>
<li>功能：修改、丢弃、新增请求或响应</li>
</ul>
</li>
<li>Spider：需要用户编写配置代码<ul>
<li>解析Downloader返回的响应（Response）</li>
<li>产生爬取项（scraped item）</li>
<li>产生额外的爬取请求（Request）</li>
</ul>
</li>
<li>Item Pipelines：需要用户编写配置代码<ul>
<li>以流水线方式处理Spider产生的爬取项</li>
<li>由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型</li>
<li>可能操作包括：清理、检验、和查重爬取项中的HTML数据、将数据存储到数据库</li>
</ul>
</li>
<li>Spider Middleware：用户可以编写配置代码<ul>
<li>目的：对请求和爬取项的再处理</li>
<li>功能：修改、丢弃、新增请求或爬取项</li>
</ul>
</li>
</ol>
<h2 id="requests-vs-Scrapy"><a href="#requests-vs-Scrapy" class="headerlink" title="requests vs. Scrapy"></a>requests vs. Scrapy</h2><ul>
<li><p>相同点</p>
<ul>
<li>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线</li>
<li>两者可用性都好，文档丰富，入门简单</li>
<li>两者都没有处理js、提交表单、应对验证码等功能（可扩展）</li>
</ul>
</li>
<li><p>不同点</p>
<table>
<thead>
<tr>
<th>requests</th>
<th>Scrapy</th>
</tr>
</thead>
<tbody><tr>
<td>页面级爬虫</td>
<td>网站级爬虫</td>
</tr>
<tr>
<td>功能库</td>
<td>框架</td>
</tr>
<tr>
<td>并发性考虑不足，性能较差</td>
<td>并发性好，性能较高</td>
</tr>
<tr>
<td>重点在于页面下载</td>
<td>重点在于爬虫结构</td>
</tr>
<tr>
<td>定制灵活</td>
<td>一般定制灵活，深度定制困难</td>
</tr>
<tr>
<td>上手十分简单</td>
<td>入门稍难</td>
</tr>
</tbody></table>
<h2 id="Scrapy爬虫的常用命令"><a href="#Scrapy爬虫的常用命令" class="headerlink" title="Scrapy爬虫的常用命令"></a>Scrapy爬虫的常用命令</h2></li>
</ul>
<p>Scrapy命令行</p>
<p>​    Scrapy是为持续运行设计的专业爬虫框架，提供操作的Scrapy命令行</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>格式</th>
</tr>
</thead>
<tbody><tr>
<td>startproject</td>
<td>创建一个新工程</td>
<td>scrapy startproject <name>[dir]</name></td>
</tr>
<tr>
<td>genspider</td>
<td>创建一个爬虫</td>
<td>scrapy genspider [options] <name> <domain></domain></name></td>
</tr>
<tr>
<td>settings</td>
<td>获得爬虫配置信息</td>
<td>scrapy setting [options]</td>
</tr>
<tr>
<td>crawl</td>
<td>运行一个爬虫</td>
<td>scrapy crawl <spider></spider></td>
</tr>
<tr>
<td>list</td>
<td>列出工程中所有爬虫</td>
<td>scrapy list</td>
</tr>
<tr>
<td>shell</td>
<td>启动URL调试命令行</td>
<td>scrapy shell [url]</td>
</tr>
</tbody></table>
<h2 id="Scrapy框架的基本使用"><a href="#Scrapy框架的基本使用" class="headerlink" title="Scrapy框架的基本使用"></a>Scrapy框架的基本使用</h2><h3 id="步骤1：建立一个Scrapy爬虫工程"><a href="#步骤1：建立一个Scrapy爬虫工程" class="headerlink" title="步骤1：建立一个Scrapy爬虫工程"></a>步骤1：建立一个Scrapy爬虫工程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>打开命令提示符-win+r 输入cmd</span><br><span class="line"><span class="meta">#</span>进入存放工程的目录</span><br><span class="line">D:\&gt;cd demo</span><br><span class="line">D:\demo&gt;</span><br><span class="line"><span class="meta">#</span>建立一个工程，工程名python123demo</span><br><span class="line">D:\demo&gt;scrapy startproject python123demo</span><br><span class="line">New Scrapy project 'python123demo', using template directory 'd:\program files\python\lib\site-packages\scrapy\templates\project', created in:</span><br><span class="line">    D:\demo\python123demo</span><br><span class="line"></span><br><span class="line">You can start your first spider with:</span><br><span class="line">    cd python123demo</span><br><span class="line">    scrapy genspider example example.com</span><br><span class="line">D:\demo&gt;</span><br></pre></td></tr></table></figure>

<p>生成的目录工程介绍：</p>
<blockquote>
<p>python123demo/          —————-&gt; 外层目录</p>
<p>​    scrapy.cfg                        ———&gt; 部署Scrapy爬虫的配置文件   </p>
<p>​    python123demo/          ———&gt; Scrapy框架的用户自定义Python代码</p>
<p>​        <code>__init__.py</code>           —-&gt; 初始化脚本</p>
<p>​        items.py                —-&gt; Items代码模板（继承类）</p>
<p>​        middlewares.py       —-&gt; Middlewares代码模板（继承类）</p>
<p>​        pipelines.py              —-&gt; Pipelines代码模板（继承类）</p>
<p>​        settings.py            —-&gt; Scrapy爬虫的配置文件</p>
<p>​        spiders/                —-&gt; Spiders代码模板目录（继承类）</p>
</blockquote>
<blockquote>
<p>spiders/            —————-&gt; Spiders代码模板目录（继承类）</p>
<p>​    <code>__init__.py</code>        ——–&gt; 初始文件，无需修改</p>
<p>​    <code>__pycache__/</code>          ——–&gt; 缓存目录，无需修改    </p>
</blockquote>
<h3 id="步骤2：在工程中产生一个Scrapy爬虫"><a href="#步骤2：在工程中产生一个Scrapy爬虫" class="headerlink" title="步骤2：在工程中产生一个Scrapy爬虫"></a>步骤2：在工程中产生一个Scrapy爬虫</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>切换到工程目录</span><br><span class="line">D:\demo&gt;cd python123demo</span><br><span class="line"><span class="meta">#</span>产生一个scrapy爬虫</span><br><span class="line">D:\demo\python123demo&gt;scrapy genspider demo python123.io</span><br><span class="line">Created spider 'demo' using template 'basic' in module:</span><br><span class="line">  python123demo.spiders.demo</span><br><span class="line"></span><br><span class="line">D:\demo\python123demo&gt;</span><br></pre></td></tr></table></figure>

<h3 id="步骤3：配置产生的spider爬虫"><a href="#步骤3：配置产生的spider爬虫" class="headerlink" title="步骤3：配置产生的spider爬虫"></a>步骤3：配置产生的spider爬虫</h3><p>修改D:\demo\python123demo\python123demo\spiders\demo.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'demo'</span></span><br><span class="line"><span class="comment">#    allowed_domains = ['python123.io']</span></span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/ws/demo.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        fname = response.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">with</span> open(fname, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Save file %s.'</span> % name)</span><br></pre></td></tr></table></figure>

<p>完整版代码编写方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"demo"</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        urls = [</span><br><span class="line">        		<span class="string">'http://python123.io/ws/demo.html'</span></span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Requests(url=url, callback=self.parse)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        fname = response.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">with</span> open(fname, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s.'</span> % fname)</span><br></pre></td></tr></table></figure>

<h3 id="步骤4：运行爬虫，获取网页"><a href="#步骤4：运行爬虫，获取网页" class="headerlink" title="步骤4：运行爬虫，获取网页"></a>步骤4：运行爬虫，获取网页</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>输入运行命令 scrapy crawl</span><br><span class="line">D:\demo\python123demo&gt;scrapy crawl demo</span><br></pre></td></tr></table></figure>

<h4 id="可能出现的错误"><a href="#可能出现的错误" class="headerlink" title="可能出现的错误"></a>可能出现的错误</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &apos;win32api&apos;</span><br></pre></td></tr></table></figure>

<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ol>
<li><p>到 <a href="https://pypi.org/project/pypiwin32/#files" target="_blank" rel="noopener">https://pypi.org/project/pypiwin32/#files</a> 下载py3版本的<a href="https://files.pythonhosted.org/packages/d0/1b/2f292bbd742e369a100c91faa0483172cd91a1a422a6692055ac920946c5/pypiwin32-223-py3-none-any.whl" target="_blank" rel="noopener">pypiwin32-223-py3-none-any.whl</a>文件；</p>
</li>
<li><p>安装pypiwin32-223-py3-none-any.whl</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install D:\download\pypiwin32-223-py3-none-any.whl</span><br></pre></td></tr></table></figure>
</li>
<li><p>再次在工程目录下运行爬虫</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl demo</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="yield关键字的使用"><a href="#yield关键字的使用" class="headerlink" title="yield关键字的使用"></a>yield关键字的使用</h2><ul>
<li>yield&lt;———————–&gt;生成器<ul>
<li>生成器是一个不断产生值的函数；</li>
<li>包含yield语句的函数是一个生成器；</li>
<li>生成器每次产生一个值（yield语句），函数会被冻结，被唤醒后再产生一个值；</li>
</ul>
</li>
</ul>
<p>实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">yield</span> i**<span class="number">2</span></span><br><span class="line"><span class="comment">#产生小于n的所有2的平方值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> gen(<span class="number">5</span>):</span><br><span class="line">    print(i, <span class="string">" "</span>, end=<span class="string">""</span>)</span><br><span class="line"><span class="comment">#0 1 4 9 16</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#普通写法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(n)</span>:</span></span><br><span class="line">    ls = [i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">return</span> ls</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> square(<span class="number">5</span>):</span><br><span class="line">    print(i, <span class="string">" "</span>, end=<span class="string">""</span>)</span><br><span class="line"><span class="comment">#0 1 4 9 16</span></span><br></pre></td></tr></table></figure>

<h3 id="为何要有生成器？"><a href="#为何要有生成器？" class="headerlink" title="为何要有生成器？"></a>为何要有生成器？</h3><ul>
<li>生成器比一次列出所有内容的优势<ul>
<li>更节省存储空间</li>
<li>响应更迅速</li>
<li>使用更灵活</li>
</ul>
</li>
</ul>
<h3 id="Scrapy爬虫的使用步骤"><a href="#Scrapy爬虫的使用步骤" class="headerlink" title="Scrapy爬虫的使用步骤"></a>Scrapy爬虫的使用步骤</h3><ul>
<li>步骤1：创建一个工程和Spider模板；</li>
<li>步骤2：编写Spider；</li>
<li>步骤3：编写Item Pipeline</li>
<li>步骤4：优化配置策略</li>
</ul>
<h2 id="Scrapy爬虫的数据类型"><a href="#Scrapy爬虫的数据类型" class="headerlink" title="Scrapy爬虫的数据类型"></a>Scrapy爬虫的数据类型</h2><h3 id="Request类"><a href="#Request类" class="headerlink" title="Request类"></a>Request类</h3><blockquote>
<p>class scrapy.http.Request()</p>
</blockquote>
<ul>
<li>Request对象表示一个HTTP请求</li>
<li>由Spider生成，由Downloader执行</li>
</ul>
<table>
<thead>
<tr>
<th>属性或方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.url</td>
<td>Request对应的请求URL地址</td>
</tr>
<tr>
<td>.method</td>
<td>对应的请求方法，’GET‘ ’POST‘等</td>
</tr>
<tr>
<td>.headers</td>
<td>字典类型风格的请求头</td>
</tr>
<tr>
<td>.body</td>
<td>请求内容主体，字符串类型</td>
</tr>
<tr>
<td>.meta</td>
<td>用户添加的扩展信息，在Scrapy内部模块间传递信息使用</td>
</tr>
<tr>
<td>.copy()</td>
<td>复制该请求</td>
</tr>
</tbody></table>
<h3 id="Response类"><a href="#Response类" class="headerlink" title="Response类"></a>Response类</h3><blockquote>
<p>class scrapy.http.Response()</p>
</blockquote>
<ul>
<li>Response对象表示一个HTTP响应</li>
<li>由Downloader生成，由Spider处理</li>
</ul>
<table>
<thead>
<tr>
<th>属性或方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.url</td>
<td>Response对应的URL地址</td>
</tr>
<tr>
<td>.status</td>
<td>HTTP状态码，默认是200</td>
</tr>
<tr>
<td>.headers</td>
<td>Response对应的头部信息</td>
</tr>
<tr>
<td>.body</td>
<td>Response对应的内容信息，字符串类型</td>
</tr>
<tr>
<td>.flags</td>
<td>一组标记</td>
</tr>
<tr>
<td>.request</td>
<td>产生Response类型对应的Request对象</td>
</tr>
<tr>
<td>.copy()</td>
<td>复制该响应</td>
</tr>
</tbody></table>
<h3 id="Item类"><a href="#Item类" class="headerlink" title="Item类"></a>Item类</h3><p>class scrapy.item.Item()</p>
<ul>
<li>Item对象表示一个从HTML页面中提取的信息内容</li>
<li>由Spider生成，由Item Pipeline处理</li>
<li>Item类似字典类型，可以按照字典类型操作</li>
</ul>
<h2 id="CSS-Selector的基本使用"><a href="#CSS-Selector的基本使用" class="headerlink" title="CSS Selector的基本使用"></a>CSS Selector的基本使用</h2><blockquote>
<html>.css('a::attr(href)').extract()
</html></blockquote>
<p>CSS Selector由W3C组织维护并规范。</p>
<h2 id="股票数据Scrapy爬虫实例"><a href="#股票数据Scrapy爬虫实例" class="headerlink" title="股票数据Scrapy爬虫实例"></a>股票数据Scrapy爬虫实例</h2><blockquote>
<p>功能描述：</p>
<ul>
<li>技术路线：scrapy</li>
<li>目标：获取上交所和深交所所有股票的名称和交易信息</li>
<li>输出：保存到文件中</li>
</ul>
</blockquote>
<h3 id="实例编写"><a href="#实例编写" class="headerlink" title="实例编写"></a>实例编写</h3><ul>
<li>步骤1：首先进入命令提示符建立工程和Spider模板</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject BaiduStocks</span><br><span class="line">cd BaiduStocks</span><br><span class="line">scrapy genspider stocks baidu.com</span><br><span class="line"><span class="meta">#</span>进一步修改spiders/stocks.py文件</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤2：编写Spider<ul>
<li>配置stock.py文件</li>
<li>修改对返回页面的处理</li>
<li>修改对新增URL爬取请求的处理</li>
</ul>
</li>
</ul>
<p>打开spider.stocks.py文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StocksSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"stocks"</span></span><br><span class="line">    start_urls = [<span class="string">'https://quote.eastmoney.com/stocklist.html'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'a::attr(href)'</span>).extract():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                stock = re.findall(<span class="string">r"[s][hz]\d&#123;6&#125;"</span>, href)[<span class="number">0</span>]</span><br><span class="line">                url = <span class="string">'https://gupiao.baidu.com/stock/'</span> + stock + <span class="string">'.html'</span></span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_stock)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_stock</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        infoDict = &#123;&#125;</span><br><span class="line">        stockInfo = response.css(<span class="string">'.stock-bets'</span>)</span><br><span class="line">        name = stockInfo.css(<span class="string">'.bets-name'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        keyList = stockInfo.css(<span class="string">'dt'</span>).extract()</span><br><span class="line">        valueList = stockInfo.css(<span class="string">'dd'</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keyList)):</span><br><span class="line">            key = re.findall(<span class="string">r'&gt;.*&lt;/dt&gt;'</span>, keyList[i])[<span class="number">0</span>][<span class="number">1</span>:<span class="number">-5</span>]</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                val = re.findall(<span class="string">r'\d+\.?.*&lt;/dd&gt;'</span>, valueList[i])[<span class="number">0</span>][<span class="number">0</span>:<span class="number">-5</span>]</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                val = <span class="string">'--'</span></span><br><span class="line">            infoDict[key]=val</span><br><span class="line">    </span><br><span class="line">        infoDict.update(</span><br><span class="line">            &#123;<span class="string">'股票名称'</span>: re.findall(<span class="string">'\s.*\('</span>,name)[<span class="number">0</span>].split()[<span class="number">0</span>] + \</span><br><span class="line">                re.findall(<span class="string">'\&gt;.*\&lt;'</span>, name)[<span class="number">0</span>][<span class="number">1</span>:<span class="number">-1</span>]&#125;)</span><br><span class="line">        <span class="keyword">yield</span> infoDict</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤3：编写Pipelines<ul>
<li>配置pipelines.py文件</li>
<li>定义对爬取项（Scrapy Item）的处理类</li>
<li>配置ITEM_PIPELINES选项</li>
</ul>
</li>
</ul>
<p>pipelines.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaidustocksPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaidustocksInfoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.f = open(<span class="string">'BaiduStockInfo.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.f.close()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            line = str(dict(item)) + <span class="string">'\n'</span></span><br><span class="line">            self.f.write(line)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>setting.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">	<span class="string">'BaiduStocks.pipelines.BaidustocksInfoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="配置并发连接选项"><a href="#配置并发连接选项" class="headerlink" title="配置并发连接选项"></a>配置并发连接选项</h3><p>settings.py</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>CONCURRENT_REQUESTS</td>
<td>Downloader最大并发请求下载数量，默认为32</td>
</tr>
<tr>
<td>CONCURRENT_ITEMS</td>
<td>Item Pipeline最大并发ITEM处理数量，默认为100</td>
</tr>
<tr>
<td>CONCURRENT_REQUESTS_PRE_DOMAIN</td>
<td>每个目标域名最大的并发请求数量，默认为8</td>
</tr>
<tr>
<td>CONCURRENT_REQUESTS_PRE_IP</td>
<td>每个目标IP最大的并发请求数量，默认为0，非0有效</td>
</tr>
</tbody></table>
<p>来源：中国大学MOOC-北京理工大学-嵩天-Python网络爬虫与信息提取</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/网络爬虫/" rel="tag"># 网络爬虫</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/02/5-27/" rel="next" title="5-27">
                <i class="fa fa-chevron-left"></i> 5-27
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/10/6-03/" rel="prev" title="6-03">
                6-03 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.jpg" alt="tassel">
            
              <p class="site-author-name" itemprop="name">tassel</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zsy0216" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://weibo.com/5480278181" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/Ep_Little_prince" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-globe"></i>CSDN</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Requests库入门"><span class="nav-number">1.</span> <span class="nav-text">1.Requests库入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Requests安装"><span class="nav-number">1.1.</span> <span class="nav-text">Requests安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HTTP协议"><span class="nav-number">1.2.</span> <span class="nav-text">HTTP协议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#URL格式"><span class="nav-number">1.2.1.</span> <span class="nav-text">URL格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#操作"><span class="nav-number">1.2.2.</span> <span class="nav-text">操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Requests主要方法"><span class="nav-number">1.3.</span> <span class="nav-text">Requests主要方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#request-方法"><span class="nav-number">1.3.1.</span> <span class="nav-text">request()方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#get-方法"><span class="nav-number">1.3.2.</span> <span class="nav-text">get()方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Response对象"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Response对象</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#head-方法"><span class="nav-number">1.3.3.</span> <span class="nav-text">head()方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#post-方法"><span class="nav-number">1.3.4.</span> <span class="nav-text">post()方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#put-方法"><span class="nav-number">1.3.5.</span> <span class="nav-text">put()方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#patch-方法"><span class="nav-number">1.3.6.</span> <span class="nav-text">patch()方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#delete-方法"><span class="nav-number">1.3.7.</span> <span class="nav-text">delete()方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Requests库的异常"><span class="nav-number">1.4.</span> <span class="nav-text">Requests库的异常</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#爬取网页的通用代码框架"><span class="nav-number">1.5.</span> <span class="nav-text">爬取网页的通用代码框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实例"><span class="nav-number">1.6.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#向百度提交关键词"><span class="nav-number">1.6.1.</span> <span class="nav-text">向百度提交关键词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#获取网络图片及存储"><span class="nav-number">1.6.2.</span> <span class="nav-text">获取网络图片及存储</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-信息提取之Beautiful-Soup库入门"><span class="nav-number">2.</span> <span class="nav-text">2.信息提取之Beautiful Soup库入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Beautiful-Soup库安装"><span class="nav-number">2.1.</span> <span class="nav-text">Beautiful Soup库安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beautiful-Soup库的基本元素"><span class="nav-number">2.2.</span> <span class="nav-text">Beautiful Soup库的基本元素</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Beautiful-Soup库的引用"><span class="nav-number">2.2.1.</span> <span class="nav-text">Beautiful Soup库的引用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beautiful-Soup类的基本元素"><span class="nav-number">2.2.2.</span> <span class="nav-text">Beautiful Soup类的基本元素</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于bs4库的HTML内容遍历方法"><span class="nav-number">2.3.</span> <span class="nav-text">基于bs4库的HTML内容遍历方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下行遍历"><span class="nav-number">2.3.1.</span> <span class="nav-text">下行遍历</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#上行遍历"><span class="nav-number">2.3.2.</span> <span class="nav-text">上行遍历</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#平行遍历"><span class="nav-number">2.3.3.</span> <span class="nav-text">平行遍历</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于bs4库的HTML格式化和编码"><span class="nav-number">2.4.</span> <span class="nav-text">基于bs4库的HTML格式化和编码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#格式化方法：-prettify"><span class="nav-number">2.4.1.</span> <span class="nav-text">格式化方法：.prettify()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编码：默认utf-8"><span class="nav-number">2.4.2.</span> <span class="nav-text">编码：默认utf-8</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-信息组织与提取"><span class="nav-number">3.</span> <span class="nav-text">3.信息组织与提取</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#信息标记的三种形式"><span class="nav-number">3.1.</span> <span class="nav-text">信息标记的三种形式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#XML-eXtensible-Matkup-Language"><span class="nav-number">3.1.1.</span> <span class="nav-text">XML: eXtensible Matkup Language</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JSON-JavaScript-Object-Notation"><span class="nav-number">3.1.2.</span> <span class="nav-text">JSON:  JavaScript Object Notation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YAMl-YAML-Ain’t-Markup-Language"><span class="nav-number">3.1.3.</span> <span class="nav-text">YAMl: YAML Ain’t Markup Language</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#信息提取的一般方法"><span class="nav-number">3.2.</span> <span class="nav-text">信息提取的一般方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一：完整解析信息的标记形式，再提取关键信息。"><span class="nav-number">3.2.1.</span> <span class="nav-text">方法一：完整解析信息的标记形式，再提取关键信息。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二：无视标记形式，直接搜索关键信息"><span class="nav-number">3.2.2.</span> <span class="nav-text">方法二：无视标记形式，直接搜索关键信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#融合方法：结合形式解析与搜索方法-提取关键信息"><span class="nav-number">3.2.3.</span> <span class="nav-text">融合方法：结合形式解析与搜索方法,提取关键信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实例：提取HTML中所有URL链接"><span class="nav-number">3.2.4.</span> <span class="nav-text">实例：提取HTML中所有URL链接</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于bs4库的HTML内容查找方法"><span class="nav-number">3.3.</span> <span class="nav-text">基于bs4库的HTML内容查找方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-信息提取实例"><span class="nav-number">4.</span> <span class="nav-text">4.信息提取实例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#中国大学排名定向爬虫"><span class="nav-number">4.1.</span> <span class="nav-text">中国大学排名定向爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初步代码编写"><span class="nav-number">4.2.</span> <span class="nav-text">初步代码编写</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#中文输出对齐问题"><span class="nav-number">4.3.</span> <span class="nav-text">中文输出对齐问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码优化"><span class="nav-number">4.4.</span> <span class="nav-text">代码优化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-实战之Re库入门"><span class="nav-number">5.</span> <span class="nav-text">5.实战之Re库入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则表达式的语法"><span class="nav-number">5.1.</span> <span class="nav-text">正则表达式的语法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#经典正则表达式实例"><span class="nav-number">5.1.1.</span> <span class="nav-text">经典正则表达式实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Re库的基本使用"><span class="nav-number">5.2.</span> <span class="nav-text">Re库的基本使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#正则表达式的表示类型"><span class="nav-number">5.2.1.</span> <span class="nav-text">正则表达式的表示类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Re库主要功能函数"><span class="nav-number">5.2.2.</span> <span class="nav-text">Re库主要功能函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#re-search-pattern-string-flags-0"><span class="nav-number">5.2.2.1.</span> <span class="nav-text">re.search(pattern,string,flags=0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#re-match-pattern-string-flags-0"><span class="nav-number">5.2.2.2.</span> <span class="nav-text">re.match(pattern,string,flags=0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#re-findall-pattern-string-flags-0"><span class="nav-number">5.2.2.3.</span> <span class="nav-text">re.findall(pattern,string,flags=0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#re-split-pattern-string-maxsplit-0-flags-0"><span class="nav-number">5.2.2.4.</span> <span class="nav-text">re.split(pattern,string,maxsplit=0,flags=0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#re-finditer-pattern-string-flags-0"><span class="nav-number">5.2.2.5.</span> <span class="nav-text">re.finditer(pattern,string,flags=0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#re-sub-pattern-repl-string-count-0-flags-0"><span class="nav-number">5.2.2.6.</span> <span class="nav-text">re.sub(pattern,repl,string,count=0,flags=0)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Re库的另一种用法"><span class="nav-number">5.2.3.</span> <span class="nav-text">Re库的另一种用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#re-compile-pattern-flags-0"><span class="nav-number">5.2.3.1.</span> <span class="nav-text">re.compile(pattern,flags=0)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Re库的match对象"><span class="nav-number">5.2.4.</span> <span class="nav-text">Re库的match对象</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Match对象的属性"><span class="nav-number">5.2.4.1.</span> <span class="nav-text">Match对象的属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Match对象的方法"><span class="nav-number">5.2.4.2.</span> <span class="nav-text">Match对象的方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Re库的贪婪匹配和最小匹配"><span class="nav-number">5.2.5.</span> <span class="nav-text">Re库的贪婪匹配和最小匹配</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Re库实例之淘宝商品比价定向爬虫"><span class="nav-number">5.3.</span> <span class="nav-text">Re库实例之淘宝商品比价定向爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Re库实例之股票数据定向爬虫"><span class="nav-number">5.4.</span> <span class="nav-text">Re库实例之股票数据定向爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初步代码编写-error"><span class="nav-number">5.4.1.</span> <span class="nav-text">初步代码编写(error)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码优化-error"><span class="nav-number">5.4.2.</span> <span class="nav-text">代码优化(error)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试成功代码"><span class="nav-number">5.4.3.</span> <span class="nav-text">测试成功代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-爬虫框架-Scrapy"><span class="nav-number">6.</span> <span class="nav-text">6.爬虫框架-Scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#安装Scrapy"><span class="nav-number">6.1.</span> <span class="nav-text">安装Scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#遇到错误"><span class="nav-number">6.1.1.</span> <span class="nav-text">遇到错误</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方案"><span class="nav-number">6.1.2.</span> <span class="nav-text">解决方案</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy爬虫框架解析"><span class="nav-number">6.2.</span> <span class="nav-text">Scrapy爬虫框架解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#requests-vs-Scrapy"><span class="nav-number">6.3.</span> <span class="nav-text">requests vs. Scrapy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy爬虫的常用命令"><span class="nav-number">6.4.</span> <span class="nav-text">Scrapy爬虫的常用命令</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy框架的基本使用"><span class="nav-number">6.5.</span> <span class="nav-text">Scrapy框架的基本使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#步骤1：建立一个Scrapy爬虫工程"><span class="nav-number">6.5.1.</span> <span class="nav-text">步骤1：建立一个Scrapy爬虫工程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#步骤2：在工程中产生一个Scrapy爬虫"><span class="nav-number">6.5.2.</span> <span class="nav-text">步骤2：在工程中产生一个Scrapy爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#步骤3：配置产生的spider爬虫"><span class="nav-number">6.5.3.</span> <span class="nav-text">步骤3：配置产生的spider爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#步骤4：运行爬虫，获取网页"><span class="nav-number">6.5.4.</span> <span class="nav-text">步骤4：运行爬虫，获取网页</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#可能出现的错误"><span class="nav-number">6.5.4.1.</span> <span class="nav-text">可能出现的错误</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解决方法"><span class="nav-number">6.5.4.2.</span> <span class="nav-text">解决方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yield关键字的使用"><span class="nav-number">6.6.</span> <span class="nav-text">yield关键字的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为何要有生成器？"><span class="nav-number">6.6.1.</span> <span class="nav-text">为何要有生成器？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy爬虫的使用步骤"><span class="nav-number">6.6.2.</span> <span class="nav-text">Scrapy爬虫的使用步骤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy爬虫的数据类型"><span class="nav-number">6.7.</span> <span class="nav-text">Scrapy爬虫的数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Request类"><span class="nav-number">6.7.1.</span> <span class="nav-text">Request类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Response类"><span class="nav-number">6.7.2.</span> <span class="nav-text">Response类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Item类"><span class="nav-number">6.7.3.</span> <span class="nav-text">Item类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CSS-Selector的基本使用"><span class="nav-number">6.8.</span> <span class="nav-text">CSS Selector的基本使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#股票数据Scrapy爬虫实例"><span class="nav-number">6.9.</span> <span class="nav-text">股票数据Scrapy爬虫实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实例编写"><span class="nav-number">6.9.1.</span> <span class="nav-text">实例编写</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置并发连接选项"><span class="nav-number">6.9.2.</span> <span class="nav-text">配置并发连接选项</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_pv"></span>
</span>
</div>

<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">tassel</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">35.2k</span>
  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("9VVhv0RPXqLP7GHimPD7BQ6Y-gzGzoHsz", "GI6gXQhWWFJIYCIXAYM627ut");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

  
  
  
</body>
</html>

<!-- 页面点击小红心和文字 -->
<script type="text/javascript" src="/js/src/click_show_love.js"></script>
<script type="text/javascript" src="/js/src/click_show_text.js"></script>